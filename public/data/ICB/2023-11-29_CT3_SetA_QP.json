{
  "paper_title": "Introduction to Computational Biology - Continuous Learning Assessment Test III",
  "source": "2023-11-29 CT3 SetA",
  "questions": [
    {
      "question_number": "1",
      "question_text": "The secondary structure prediction methods can be either Abinitio or _________ networks, (b)sequence based (c)machine learning (d) homology based",
      "marks": 1,
      "answer": "The missing word is (a) neural. Secondary structure prediction methods can be either Abinitio or neural networks, sequence-based, machine learning, or homology-based.",
      "answer_source": "generated",
      "chapter": "Unit 3: Structure Biology"
    },
    {
      "question_number": "2",
      "question_text": "Which among these is odd (a) space fill (b) ribbon (c) side chain model (d)bat and ball",
      "marks": 1,
      "answer": "(d) bat and ball. Space fill, ribbon, and side chain models are common representations used in protein visualization tools. 'Bat and ball' is not a standard visualization model in this context, it's likely a typo for 'ball and stick' which is another common model.",
      "answer_source": "generated",
      "chapter": "Unit 3: Structure Biology"
    },
    {
      "question_number": "3",
      "question_text": "The anticodon for AAU is (a)TGA (b) GAT (c) UGA (d) UUA",
      "marks": 1,
      "answer": "(d) UUA. The mRNA codon is AAU. During translation, the tRNA anticodon base pairs with the mRNA codon. Adenine (A) pairs with Uracil (U), and Uracil (U) pairs with Adenine (A). Therefore, for AAU, the anticodon is UUA.",
      "answer_source": "generated",
      "chapter": "Unit 3: Structure Biology"
    },
    {
      "question_number": "4",
      "question_text": "Clustering is a type of _________ machine learning algorithm (a)supervised (b) unsupervised (c) reciprocal (d) hierarchical",
      "marks": 1,
      "answer": "(b) unsupervised. Clustering is a type of unsupervised machine learning algorithm, meaning it works with unlabeled data to find inherent groupings or patterns.",
      "answer_source": "generated",
      "chapter": "Unit 4: Neurobiology"
    },
    {
      "question_number": "5",
      "question_text": "Deposits of _________ interfere with neuronal functions in Alzheimer's (a) plaques (B)dopamine (c)amino acids (d)",
      "marks": 1,
      "answer": "(a) plaques. Deposits of amyloid-beta plaques (and neurofibrillary tangles) are characteristic pathological hallmarks that interfere with neuronal functions in Alzheimer's disease.",
      "answer_source": "generated",
      "chapter": "Unit 4: Neurobiology"
    },
    {
      "question_number": "6",
      "question_text": "The spiking period is followed by a _________ period (a)refractory (b) resolution (c) unspike (d)silent",
      "marks": 1,
      "answer": "(a) refractory. In neuroscience, after a neuron generates an action potential (spiking period), there is a brief refractory period during which it is less likely or unable to fire another action potential.",
      "answer_source": "generated",
      "chapter": "Unit 4: Neurobiology"
    },
    {
      "question_number": "7",
      "question_text": "Albert Einstein had inordinate number of _________ cells (a) nerve (b) glial (c) liver (d)heart",
      "marks": 1,
      "answer": "(b) glial. Studies on Albert Einstein's brain have suggested an unusually high number of glial cells (specifically astrocytes) in certain regions compared to control brains, which has been hypothesized to contribute to his cognitive abilities.",
      "answer_source": "generated",
      "chapter": "Unit 4: Neurobiology"
    },
    {
      "question_number": "8",
      "question_text": "MHC is present on _________ (a) human cells (b)bacteria (c) virus (d)protozoa",
      "marks": 1,
      "answer": "(a) human cells. Major Histocompatibility Complex (MHC) molecules are cell surface proteins found on most nucleated cells in vertebrates, including human cells. They play a crucial role in immune system recognition of self versus non-self.",
      "answer_source": "generated",
      "chapter": "Unit 5: Immunobiology"
    },
    {
      "question_number": "9",
      "question_text": "_________ is a agranular leukocytes (a) basophil (b) eosinophil (c) monocyte (d)neutrophil",
      "marks": 1,
      "answer": "(c) monocyte. Monocytes are agranular leukocytes, along with lymphocytes. Basophils, eosinophils, and neutrophils are granular leukocytes.",
      "answer_source": "generated",
      "chapter": "Unit 5: Immunobiology"
    },
    {
      "question_number": "10",
      "question_text": "Diphtheria and tetanus are developed as _________ vaccine (a) peptide (b)live (c)toxoid (d)attenuated",
      "marks": 1,
      "answer": "(c) toxoid. Diphtheria and tetanus vaccines are toxoid vaccines, which are made from inactivated bacterial toxins (toxoids) that induce an immune response against the toxin rather than the bacterium itself.",
      "answer_source": "generated",
      "chapter": "Unit 5: Immunobiology"
    },
    {
      "question_number": "11",
      "question_text": "Write on cell mediated immunity",
      "marks": 10,
      "answer": "Cell-mediated immunity (CMI) is an immune response that does not involve antibodies but rather involves the activation of phagocytes, antigen-specific cytotoxic T-lymphocytes, and the release of various cytokines in response to an antigen. It primarily protects against intracellular pathogens (viruses, bacteria within host cells), cancer cells, and transplanted tissues.\n\nKey components and processes:\n1.  **T Lymphocytes (T cells):** The central players in CMI. T cells mature in the thymus and are categorized into different subsets:\n    *   **Helper T cells (CD4+ T cells):** Recognize antigens presented by MHC class II molecules on antigen-presenting cells (APCs) like dendritic cells, macrophages, and B cells. Upon activation, they proliferate and secrete cytokines that stimulate other immune cells, including B cells (for antibody production) and cytotoxic T cells (for cell killing). Key cytokines include interleukins (e.g., IL-2, IL-4, IL-5) and interferon-gamma (IFN-γ).\n    *   **Cytotoxic T lymphocytes (CTLs or CD8+ T cells):** Recognize antigens presented by MHC class I molecules, found on nearly all nucleated cells. When activated (often with help from CD4+ T cells), CTLs become killer cells that directly identify and destroy infected cells or cancer cells by inducing apoptosis (programmed cell death). They achieve this by releasing perforin (which forms pores in the target cell membrane) and granzymes (proteases that enter the target cell and trigger apoptosis).\n    *   **Regulatory T cells (Tregs):** Suppress immune responses to maintain self-tolerance and prevent autoimmune diseases.\n\n2.  **Antigen Presentation:** For T cells to recognize an antigen, it must be processed and presented on the surface of an APC in association with MHC molecules. \n    *   Exogenous antigens (from outside the cell) are processed and presented via MHC class II molecules to CD4+ T cells.\n    *   Endogenous antigens (from within the cell, e.g., viral proteins or tumor antigens) are processed and presented via MHC class I molecules to CD8+ T cells.\n\n3.  **Activation and Clonal Expansion:** When a naive T cell encounters its specific antigen presented by an APC, and receives co-stimulatory signals, it becomes activated. This leads to clonal expansion, where the T cell proliferates rapidly, creating many effector and memory T cells.\n\n4.  **Effector Functions:**\n    *   CD8+ CTLs directly kill target cells.\n    *   CD4+ Helper T cells activate macrophages to become more effective at phagocytosis and killing of intracellular pathogens, and enhance the activity of CTLs and B cells.\n\n5.  **Memory T cells:** After the infection is cleared, a population of long-lived memory T cells remains. These cells allow for a faster, stronger, and more effective immune response upon subsequent exposure to the same antigen.\n\nIn summary, cell-mediated immunity is crucial for defending against pathogens that reside inside host cells and for immune surveillance against cancer.",
      "answer_source": "generated",
      "chapter": "Unit 5: Immunobiology"
    },
    {
      "question_number": "12",
      "question_text": "Explain in detail about artificial neural networks",
      "marks": 10,
      "answer": "Artificial Neural Networks (ANNs) are computational models inspired by the structure and function of biological neural networks in the human brain. They are designed to recognize patterns, classify data, and make predictions, making them fundamental to the field of artificial intelligence and machine learning.\n\n**1. Basic Structure:**\nAn ANN consists of interconnected nodes (neurons or units) organized in layers:\n*   **Input Layer:** Receives raw data. Each node represents an input feature.\n*   **Hidden Layers:** One or more layers between the input and output layers where computations are performed. These layers extract increasingly abstract features from the data.\n*   **Output Layer:** Produces the network's final prediction or classification.\n\n**2. Neurons (Nodes):**\nEach neuron in a hidden or output layer performs two main operations:\n*   **Weighted Sum:** It receives inputs from other neurons, each multiplied by an associated 'weight'. These weighted inputs are summed up along with a 'bias' term (a constant value).\n    *   `Z = (Input1 * Weight1) + (Input2 * Weight2) + ... + Bias`\n*   **Activation Function:** The sum `Z` is then passed through a non-linear activation function (e.g., Sigmoid, ReLU, Tanh). This function introduces non-linearity, enabling the network to learn complex patterns and make decisions that are not just linear combinations of its inputs. The output of the activation function is the output of the neuron, which then serves as an input to neurons in the next layer.\n\n**3. Weights and Biases:**\n*   **Weights:** Represent the strength of the connection between neurons. They are initially random and are adjusted during training.\n*   **Biases:** Allow the activation function to be shifted, providing more flexibility for the network to fit the data.\n\n**4. Learning Process (Training):**\nANNs learn from data through a process called training:\n*   **Forward Propagation:** Input data is fed into the network, processed through the hidden layers, and an output is generated.\n*   **Loss Function (Cost Function):** The network's output is compared to the actual target output (for supervised learning), and a loss function calculates the error or discrepancy between the prediction and the truth. Common loss functions include Mean Squared Error for regression and Cross-Entropy for classification.\n*   **Backpropagation:** This is the core algorithm for training ANNs. It calculates the gradient of the loss function with respect to each weight and bias in the network. The gradients indicate how much each weight/bias contributed to the error.\n*   **Optimization Algorithm (e.g., Gradient Descent):** Using the gradients, an optimizer adjusts the weights and biases in small steps to minimize the loss function. This iterative process of forward propagation, loss calculation, backpropagation, and weight/bias adjustment continues over many 'epochs' (passes through the entire training dataset) until the network's performance converges or reaches an acceptable level.\n\n**5. Types of ANNs:**\n*   **Feedforward Neural Networks (FNNs):** Information flows in one direction, from input to output, without loops. These are the most basic type.\n*   **Convolutional Neural Networks (CNNs):** Primarily used for image processing and computer vision tasks. They use convolutional layers to automatically learn spatial hierarchies of features.\n*   **Recurrent Neural Networks (RNNs):** Designed for sequential data (like time series or natural language) as they have loops that allow information to persist, giving them a 'memory' of previous inputs.\n*   **Generative Adversarial Networks (GANs):** Consist of two competing networks (generator and discriminator) to generate new, realistic data.\n\n**6. Applications in Biology:**\nANNs have diverse applications in biology and bioinformatics, including:\n*   **Protein Structure Prediction:** Predicting secondary and tertiary structures.\n*   **Gene Expression Analysis:** Classifying cell types or disease states based on gene expression profiles.\n*   **Drug Discovery:** Predicting drug-target interactions or properties of potential drug candidates.\n*   **Biomarker Identification:** Identifying patterns in 'omics' data for disease diagnosis or prognosis.\n*   **Medical Image Analysis:** Detecting diseases like cancer from MRI or X-ray images.\n\nIn essence, ANNs are powerful tools capable of learning intricate relationships within data, making them highly effective for solving complex problems across various scientific and engineering domains, including computational biology.",
      "answer_source": "generated",
      "chapter": "Unit 4: Neurobiology"
    },
    {
      "question_number": "13",
      "question_text": "Give details of secondary structure prediction",
      "marks": 10,
      "answer": "Protein secondary structure prediction is a computational method that aims to predict the local three-dimensional structure of a protein (specifically, alpha-helices, beta-strands, and random coils/turns) based solely on its amino acid sequence. This is an important step in understanding protein function and can aid in predicting the full 3D tertiary structure.\n\n**1. Types of Secondary Structures:**\n*   **Alpha-helices (α-helices):** Right-handed helical structures stabilized by hydrogen bonds between the carbonyl oxygen of one amino acid and the amino hydrogen of an amino acid four positions later along the polypeptide chain.\n*   **Beta-strands (β-strands) / Beta-sheets (β-sheets):** Extended polypeptide chains that align side-by-side to form a sheet-like structure, stabilized by hydrogen bonds between adjacent strands. Can be parallel or anti-parallel.\n*   **Turns/Coils:** Irregular, non-repeating structures that connect alpha-helices and beta-strands, allowing the polypeptide chain to change direction.\n\n**2. Importance of Secondary Structure Prediction:**\n*   **Intermediate Step:** Secondary structure provides an important intermediate level of detail between primary sequence and tertiary structure, which is harder to predict directly.\n*   **Functional Insights:** Specific secondary structure motifs are often associated with particular protein functions.\n*   **Drug Design:** Understanding secondary structure can inform rational drug design.\n*   **Evolutionary Studies:** Conserved secondary structures can indicate evolutionary relationships.\n\n**3. Historical and Modern Prediction Methods:**\nEarly methods relied on statistical propensities of amino acids, while modern methods heavily utilize machine learning.\n\n*   **a. Statistical Methods (e.g., Chou-Fasman, GOR method):**\n    *   **Chou-Fasman (1974):** This method calculates the propensity of each amino acid to be in an alpha-helix, beta-strand, or turn based on empirical observations from known protein structures. It then scans the sequence, identifying segments with high propensities for a particular structure and extending them until a 'breaker' residue is encountered. It has a relatively low accuracy (around 50-60%).\n    *   **Garnier-Osguthorpe-Robson (GOR) Method (1978):** A more sophisticated statistical method that considers not just individual amino acid propensities but also the conformational preferences of short oligopeptides (e.g., a window of 8-17 residues). It uses information theory to evaluate the probability of a central residue being in a specific conformation given the context of its neighbors. GOR methods improved accuracy to around 60-65%.\n\n*   **b. Machine Learning Methods (e.g., Neural Networks, Support Vector Machines):**\n    *   **Neural Networks (NNs):** These are currently the most successful methods. NNs are trained on large datasets of known protein structures (PDB). They take a window of amino acids (typically 15-25 residues) as input, and the hidden layers learn complex patterns that correlate amino acid sequences with secondary structures. Early NN methods (e.g., PHD, 1993) achieved around 70-75% accuracy.\n    *   **Evolutionary Information / Profile-based Methods:** A major breakthrough came with incorporating evolutionary information. Instead of just using the amino acid sequence, multiple sequence alignments (MSAs) are generated, and a profile (position-specific scoring matrix, PSSM) is used as input to the NNs. PSSMs capture the variability at each position in an alignment, indicating evolutionary conservation. This approach (e.g., PSIPRED, 1999; JPred, 2000s) significantly boosted accuracy to 75-80% or higher. The idea is that conserved residues are more critical for structure/function, and variable residues can tolerate substitutions without altering the secondary structure.\n    *   **Deep Learning:** Recent advancements with deep learning architectures (e.g., protein language models) have further pushed the accuracy boundaries, reaching over 80-85% for alpha-helix/beta-strand prediction by learning even more intricate relationships and larger contextual information.\n\n**4. Evaluation of Prediction Accuracy:**\n*   **Q3 Score:** The most common metric, representing the percentage of residues correctly predicted as alpha-helix, beta-strand, or coil.\n*   **SOV (Segment Overlap Measure):** A more stringent measure that evaluates the accuracy of predicting entire segments of secondary structure, not just individual residues.\n\nSecondary structure prediction, especially with machine learning and evolutionary information, remains a vital tool in bioinformatics for gaining initial insights into protein structure when experimental data is unavailable.",
      "answer_source": "generated",
      "chapter": "Unit 3: Structure Biology"
    },
    {
      "question_number": "14",
      "question_text": "Write about translation in detail",
      "marks": 10,
      "answer": "Translation is the process by which the genetic information encoded in messenger RNA (mRNA) is decoded to synthesize proteins. This fundamental process occurs in the cytoplasm of cells on structures called ribosomes and involves various types of RNA and protein factors.\n\n**Key Components of Translation:**\n1.  **mRNA (Messenger RNA):** Carries the genetic code from DNA in the form of codons (sequences of three nucleotides) that specify particular amino acids.\n2.  **Ribosomes:** Complex molecular machines composed of ribosomal RNA (rRNA) and ribosomal proteins. They provide the platform for mRNA and tRNA interaction and catalyze peptide bond formation. Ribosomes have a small subunit (binds mRNA) and a large subunit (has catalytic activity) and three binding sites for tRNA: the A (aminoacyl) site, the P (peptidyl) site, and the E (exit) site.\n3.  **tRNA (Transfer RNA):** Small RNA molecules, each carrying a specific amino acid at one end and an anticodon (a three-nucleotide sequence complementary to an mRNA codon) at the other. tRNA molecules act as adaptors, linking codons to their corresponding amino acids.\n4.  **Amino Acids:** The building blocks of proteins.\n5.  **Aminoacyl-tRNA Synthetases:** Enzymes that attach the correct amino acid to its specific tRNA molecule, a process called 'charging' or 'aminoacylation' of tRNA.\n6.  **Initiation, Elongation, and Termination Factors:** Various protein factors that regulate the different stages of translation.\n\n**Stages of Translation:**\n\n**1. Initiation:**\n*   **Prokaryotes:** The small ribosomal subunit binds to a specific sequence on the mRNA called the Shine-Dalgarno sequence, located upstream of the start codon (AUG). An initiator tRNA (carrying formylmethionine, fMet) binds to the start codon in the P site. The large ribosomal subunit then joins, completing the initiation complex.\n*   **Eukaryotes:** The small ribosomal subunit, along with initiator tRNA (carrying methionine, Met) and several initiation factors, binds to the 5' cap of the mRNA. This complex then scans the mRNA in the 3' direction until it finds the start codon (AUG), often within a specific sequence context (Kozak sequence). Once AUG is found, the large ribosomal subunit joins, and initiation factors are released.\n\n**2. Elongation:**\nThis is a cyclical process where amino acids are added one by one to the growing polypeptide chain.\n*   **Codon Recognition:** A new aminoacyl-tRNA (carrying the next amino acid) enters the A site of the ribosome, where its anticodon base-pairs with the mRNA codon.\n*   **Peptide Bond Formation:** The ribosome's large subunit (specifically, the peptidyl transferase activity of its rRNA) catalyzes the formation of a peptide bond between the amino acid in the A site and the growing polypeptide chain held by the tRNA in the P site. This transfers the polypeptide to the tRNA in the A site.\n*   **Translocation:** The ribosome moves one codon along the mRNA in the 3' direction. This shifts the tRNA with the growing polypeptide from the A site to the P site, and the now uncharged tRNA from the P site to the E (exit) site. The E site tRNA is then released, making the A site available for the next aminoacyl-tRNA.\n*   This cycle repeats for each codon in the mRNA sequence.\n\n**3. Termination:**\n*   Elongation continues until the ribosome encounters a stop codon (UAA, UAG, or UGA) on the mRNA. There are no tRNAs that recognize stop codons.\n*   Release factors (proteins) bind to the stop codon in the A site.\n*   The release factors catalyze the hydrolysis of the bond between the polypeptide and the tRNA in the P site, releasing the completed polypeptide chain from the ribosome.\n*   The ribosomal subunits then dissociate from the mRNA and from each other, ready to initiate another round of translation.\n\n**Post-Translational Modifications:**\nAfter synthesis, proteins often undergo various modifications (e.g., folding, cleavage, glycosylation, phosphorylation) to become functional, be targeted to specific cellular locations, or be regulated.\n\nTranslation is a highly regulated and energy-intensive process, critical for all cellular functions as proteins carry out virtually all cellular tasks.",
      "answer_source": "generated",
      "chapter": "Unit 3: Structure Biology"
    },
    {
      "question_number": "15",
      "question_text": "Design and draw a neural network that would predict breast cancer taking into account the risk factors.",
      "marks": 10,
      "answer": "Designing a neural network to predict breast cancer based on risk factors involves defining the input features, the network architecture, and the output. Below is a conceptual design and a description, as a drawing cannot be provided in text format, but can be easily visualized.\n\n**Neural Network Design for Breast Cancer Prediction**\n\n**Goal:** Predict the likelihood or presence of breast cancer (classification) or its progression (regression) based on various risk factors.\n\n**1. Input Layer:**\nThis layer will receive the numerical representation of the identified risk factors. Each neuron in the input layer corresponds to one specific risk factor. Examples of relevant risk factors include:\n*   **Age:** (Numerical) Patient's age.\n*   **Family History:** (Binary/Categorical) Presence/absence of breast cancer in close relatives (e.g., mother, sister, daughter).\n*   **Genetic Mutations:** (Binary/Categorical) Presence/absence of BRCA1/BRCA2 mutations.\n*   **Breast Density:** (Categorical/Numerical) Measured breast density from mammography.\n*   **Hormone Exposure:** (Categorical/Numerical) Early menarche, late menopause, hormone therapy use, parity (number of full-term pregnancies).\n*   **Lifestyle Factors:** (Categorical/Numerical) Alcohol consumption, smoking status, obesity (BMI), physical activity level.\n*   **Biopsy Results:** (Categorical/Numerical) Atypical hyperplasia, Lobular carcinoma in situ (LCIS), or other pre-cancerous conditions if available.\n*   **Prior Radiation Exposure:** (Binary) History of radiation therapy to the chest.\n\n*   **Representation:** Categorical variables would be one-hot encoded (e.g., for 'Family History: Yes/No', two input neurons: [1,0] or [0,1]). Numerical features should be normalized (e.g., min-max scaling or z-score standardization) to a range like [0, 1] or mean 0, std dev 1.\n\n**2. Hidden Layers:**\n*   **Number of Layers:** Typically 2 to 4 hidden layers are sufficient for many complex biological problems. More layers allow the network to learn more abstract and hierarchical features from the input data.\n*   **Number of Neurons per Layer:** This is a hyperparameter often determined experimentally. A common starting point is to have neurons that are a fraction or multiple of the input layer size. For example, decreasing number of neurons in successive layers, or keeping it relatively constant.\n*   **Activation Functions:** Rectified Linear Unit (ReLU) is a popular choice for hidden layers due to its computational efficiency and ability to mitigate the vanishing gradient problem. `f(x) = max(0, x)`.\n*   **Purpose:** These layers will learn complex, non-linear relationships and interactions between the various risk factors. For instance, a combination of age, breast density, and family history might strongly indicate risk, and the hidden layers would learn to recognize such patterns.\n\n**3. Output Layer:**\n*   **Number of Neurons:**\n    *   For binary classification (e.g., 'Cancer Present' or 'Cancer Absent'), one neuron is typically used.\n*   **Activation Function:**\n    *   For a single output neuron in binary classification, the Sigmoid function (`σ(x) = 1 / (1 + e^-x)`) is used. It squashes the output to a range between 0 and 1, which can be interpreted as the probability of having breast cancer.\n    *   If multiple classes (e.g., 'No Cancer', 'Benign Tumor', 'Malignant Tumor'), a Softmax activation function would be used with multiple output neurons, where each neuron represents the probability of belonging to a specific class.\n\n**Conceptual Drawing (Text Representation):**\n\nInput Layer (Risk Factors)\n(e.g., Age, Family_History_BRCA, Breast_Density, BMI, etc.)\n[Input1] --- Weight --- [Hidden1_1]\n[Input2] --- Weight --- [Hidden1_2]\n...                  ...\n[InputN] --- Weight --- [Hidden1_M]\n                       / | \\\n                      /  |  \\\n                     /   |   \\\n              (Hidden Layer 1)\n                ReLU Activation\n                     / | \\\n                    /  |  \\\n                   /   |   \\\n           (Hidden Layer 2, etc.)\n               ReLU Activation\n                   /   \\\n                  /     \\\n                 /       \\\n               [Output Neuron]\n                Sigmoid Activation\n                (Probability of Breast Cancer)\n\n**Training Process:**\n1.  **Data Collection:** A large, diverse dataset of patient records including all the chosen risk factors and the confirmed breast cancer status (positive/negative) is required.\n2.  **Data Preprocessing:** Cleaning, handling missing values, encoding categorical variables, and scaling numerical features.\n3.  **Splitting Data:** Divide into training, validation, and test sets.\n4.  **Loss Function:** Binary Cross-Entropy is commonly used for binary classification tasks.\n5.  **Optimizer:** Adam or RMSprop are suitable adaptive optimization algorithms.\n6.  **Backpropagation:** The network adjusts its weights and biases based on the calculated gradients of the loss function to minimize prediction errors.\n7.  **Evaluation:** Performance is assessed on the test set using metrics like accuracy, precision, recall, F1-score, and AUC-ROC curve.\n\nThis neural network would learn to identify complex patterns and correlations within the risk factors, allowing it to provide a probabilistic prediction of breast cancer presence, assisting in early detection and personalized risk assessment.",
      "answer_source": "generated",
      "chapter": "Unit 4: Neurobiology"
    }
  ]
}